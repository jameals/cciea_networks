---
title: "Process Fish Tickets for Participation Networks"
author: M. Fisher, J.F. Samhouri
date: "Written Dec. 7, 2020. Last Run `r Sys.Date()`"
output:
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '3'
  pdf_document:
    highlight: haddock
    number_sections: yes
    toc: yes
    toc_depth: '3'
geometry: margin=1in
subtitle: Preparation for network analysis in CCIEA ESR 2026
fontsize: 11pt
---


# Description

This code processes raw [PacFIN](www.pacfin.pmsfc.org) fish tickets, which will then be used to generate participation networks. It is designed to write out a separate file for each year of input data, by calendar and crab year.

This version of the script will replace PacFIN's nominal species IDs with the equivalent regular species IDs. For more on nominal IDs, see the [PacFIN FAQs page](https://pacfin.psmfc.org/data/faqs/).


Additional Resources: 
* [PMSC Fleet Report](https://www.psmfc.org/efin/docs/fleetreport.pdf), esp Table 8
* [PacFIN Column names key](https://pacfin.psmfc.org//wp-content/uploads/2016/06/PacFIN_Comprehensive_Fish_Tickets.pdf)

<br>
<br>

```{r "setup", include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

if(!require("here")) {install.packages("here")}
if(!require("tidyverse")) {install.packages("tidyverse")}
if(!require("foreign")) {install.packages("foreign")}
if(!require("lubridate")) {install.packages("lubridate")}
if(!require("magrittr")) {install.packages("magrittr")}
if(!require("fredr")) {install.packages("fredr")}
if(!require("pak")) {
  install.packages("pak")
  pak::pak("keyring")
  }

## start time for full script
script_start_time <- Sys.time()

# ggplot theme
plot_theme <-   theme_minimal()+
  theme(text=element_text(family="sans",size=12,color="black"),
        legend.text = element_text(size=14),
        axis.title=element_text(family="sans",size=14,color="black"),
        axis.text=element_text(family="sans",size=8,color="black"),
        panel.grid.major = element_line(color="gray50",linetype=3))
theme_set(plot_theme)
```


This script requires the following packages. 
```{r packages, message=FALSE, warning=FALSE}
library(here)
library(tidyverse)
library(foreign)
library(lubridate)
library(fredr)
library(keyring)
```
<br>

And calls the following functions. 
```{r}
# source(here::here('R','get_vessel_length.R'))
# source(here::here('R','lengths_2yr_median.R'))
# source(here::here('R','get_mode.R'))
# source(here::here('R','lengths_5yr_registration.R'))
```
<br>

# User Inputs 

Select your directories. 
```{r get_dir}
## raw PacFIN fish ticket data file location
#indir1 <- "/Users/jameal.samhouri/Documents/Koehn et al. cc vulnerability/Data/"
indir2 <- "/Users/jameal.samhouri/Documents/CONFIDENTIAL DATA/PacFIN Tickets/"

## output directory for confidential data
out_conf_dir <- "/Users/jameal.samhouri/Documents/Github repos/CCIEA Networks/processed/"

## output directory for processed fish tickets. not confidential
processdir <- "data/input"
```
<br>

If you have a new download of fish ticket data and want to clean it up a bit and convert it to RDS, use this chunk.
```{r csv2rds}

# NOTE: reading in PacFIN csv's for multiple years is SLOW, and took >1.5hrs on JS macbook

# tmpfile <- 'fish tickets 2016-2023 121323.csv' # latest pull from PacFIN 12-13-2023
# tmpfile2 <- read.csv(paste0(indir2,tmpfile))
# glimpse(tmpfile2)
# 
# #as.Date(tmpfile2[1,]$LANDING_DATE, "%m/%d/%Y")
# 
# # # LANDING_YEAR in more recent data pulls has a time associated with it. convert myfile2$LANDING_DATE to date formats that exclude time. added 12-29-2021
# tmpfile2$LANDING_DATE <- as.Date(tmpfile2$LANDING_DATE, "%m/%d/%Y")
# write_rds(tmpfile2, paste0(out_conf_dir,'fish tickets 2016-2023 121323.rds'))

```

Specify file name of the raw fish ticket data file and supporting files with species and port grouping keys. This script takes a flexible input file name, but will write out hard-coded file names for consistency through the rest of the analysis.
```{r get_filenames}
# will need to glue 2004-2018 data to 2019-2020 data

# myfile1 <- "pacfin_compiled_2004thru2018.rds"
# myfile2 <- "fish tickets 2019-2021 121321.csv" # latest pull from PacFIN: 12-13-21. previous pulls 03-01-21, 01-27-21, and 12-07-20
# myfile2 <- "fish tickets 2004-2021 122921.csv" # latest pull from PacFIN: 12-29-21.
# myfile2 <- "pacfin_compiled_2004thru2021.rds" # latest pull from PacFIN: 12-29-21.
# myfile2 <- 'fish tickets 2004-2024 121724.rds' # latest pull from PacFIN 12-17-2024, shared by Abigail Golden

myfile2 <- 'fish tickets 2003-2025 12152025.rds' # latest pull from PacFIN 12-15-2025, shared by Abigail Golden

myfile3 <- "iopac_conversion_table.csv"
myfile4 <- "species_groupings_spgrpn2_iea.csv" # originally from first tab of species_groupings IEA.xlsx from Dan Holland. see email of 121520
# myfile4 <- "species_groupings_spgrpn2.csv" # originally from first tab of species_groupings.xlsx from Dan Holland

```
<br>


Set a few more user inputs to process the fish tickets
<br>
*If you decide to write out the processed fish ticket data using crab years as time increments (`crab_year=TRUE`), be sure to supply all of the necessary calendar years. For example, if you are processing fish tickets for the 2014 crab year, you will need to read in raw PacFIN fish ticket data for the calendar years 2013 and 2014; if you are processing fish tickets for the 2014 and 2015 crab years, you will need to read in raw PacFIN fish ticket data for calendar years 2014,2015,2016.*
```{r inputs}
## do you want to write out the fish ticket data as crab years? [TRUE/FALSE]
crab_year_write_out_trigger = TRUE

## if crab_year = TRUE, which crab years of fish tickets are you processing? [numeric vector]
crab_years <- seq(2004,2025)

## which years of fish tickets are you processing? [numeric vector]
years <- seq(2004,2025) 

## do you want to write out the average price per pounds calculated from the data? [TRUE/FALSE]
write_ppp = FALSE

## do you want to only get vessel lengths for Dungeness crab vessels (`dcrb_only` set to TRUE)?
dcrb_only <- FALSE
```
<br>
<br>


# 1: Edit PacFIN Data

## 1.1. Read, Glue, Subset data

This should be a `.rds` or a `.csv` file containing raw PacFIN data.
```{r rawdata}
script_start_time_readtix <- Sys.time()

rawdat <- read_rds(paste0(indir2,myfile2)) # 2004-2025 fish tix take ~1.2min to read on jameal's macbook pro
#glimpse(rawdat) # commented out to avoid showing confidential data
colnames(rawdat)

port_grps <- read_csv(here::here(processdir,myfile3))
sp_grps <- read_csv(here::here(processdir,myfile4)) %>% distinct() # remove duplicate rows (GPH1 is duplicated)

Sys.time()-script_start_time_readtix

```
<br>

Drop fish tix from any years not used for this analysis.
```{r smaller}

rawdat.sub0 <- rawdat %>%
  filter(LANDING_YEAR %in% years)

```

Subset the raw data to include only the columns that are needed. Then rename the columns that will be retained in the final processed data. The last three columns are used to calculate per-species / total revenue and landed weight, but will not ultimately be retained.
```{r subset}
rawdat.sub <- select(rawdat.sub0, c(FISH_TICKET_ID, PACFIN_PORT_CODE, PACFIN_GROUP_PORT_CODE, VESSEL_NUM, AGENCY_CODE, COUNCIL_CODE, 
                               GEAR_CODE, PACFIN_GROUP_GEAR_CODE, REMOVAL_TYPE_NAME, REMOVAL_TYPE_CODE, DEALER_NUM, FLEET_CODE, PARTICIPATION_GROUP_CODE,
                               LANDING_DATE, LANDING_YEAR, 
                               PACFIN_SPECIES_CODE, LANDED_WEIGHT_LBS, PRICE_PER_POUND, AFI_PRICE_PER_POUND, EXVESSEL_REVENUE, AFI_EXVESSEL_REVENUE))

colnames(rawdat.sub) <- c("trip_id","pcid", "pcgroup","drvid", "agid", "council",
                          "grid","grgroup","removal_type", "removal_type_code","proc", "fleet", "participation_group",
                          "tdate", "year", 
                          "spid", "pounds", "ppp", "afi_ppp","revenue", "afi_revenue")

#glimpse(rawdat.sub) # commented out to avoid showing confidential data

```
<br>

```{r include=FALSE}
## clean up space
rm(rawdat)
# rm(pacfin.df, rawdat1, rawdat2, pacfin2019_2020.df)
```
<br>

Remove fish tickets with an unknown (`UNKNOWN`) OR MISSING (`""` / `MISSING`) vessel identifier,  tickets where `COUNCIL_CODE == N` (fish landed in AK and sold on the West Coast), tickets where `PARTICIPATION_GROUP_CODE != "A"` (aquaculture), and tickets where `FLEET_CODE != "TI"` (tribal vessels, where many vessels do not have IDs and are assigned random numbers beginning `ZZ` (`council == "*"`)). Recommended by Erin Steiner 12/19/2024. 

As of December 2025, this results in dropping about 2.2M records. About 6.75M records remain.
```{r filter}
nrows_pre <- dim(rawdat.sub)
rawdat.sub <- rawdat.sub %>%
  filter(drvid != "UNKNOWN") %>%
  filter(drvid != "") %>%
  filter(drvid != "MISSING") %>%
  filter(council != 'N') %>% # fish caught in AK
  filter(participation_group != 'A') %>% #aquaculture 
  filter(fleet != 'TI') # tribal
nrows_post <- dim(rawdat.sub)
nrows_pre - nrows_post
nrows_post
```
<br>


## 1.2. Edit, Add columns

First, edit and add columns which describe the fish ticket date. 
```{r edit_columns}
rawdat.sub1 <- rawdat.sub %>%
  ## create combined vessel ID / year variable
  mutate(drvid_year = paste0(drvid,"_", year)) %>%
  ## create 'date' object for ticket landing date
  #mutate(tdate = as.Date(as.character(tdate), "%Y-%b-%d")) %>% # no longer needed because of joins in 'glue' chunk above. 12-29-2021 
  mutate(tdate = ymd(tdate)) %>%
  ## create 'calendar week of landing' variable
  mutate(tweek=week(tdate))

## QC: check first and last date of landing for each year, and total tickets per year. 
rawdat.sub1 %>% group_by(year) %>%
  summarise(first_ticket = min(tdate), last_ticket = max(tdate), total_tickets = length(unique(trip_id)))
# note: on 12-29-2021, jameal noticed that 2016 fish tickets from 9/20/2016 thru 12/31/2016 are excluded. that likely affected previous network graphs, but not the results presented in the 2021 ESR. Only time series considered in the presentation to the SSC-ES in Sep 2021 would be affected. To avoid issues, Jameal downloaded all fish tickets from 2004 through 12-29-2021 on 12-29-2021 into a single file.
```
<br>

```{r include=FALSE}
## clean up space
rm(rawdat.sub)
```
<br>

Next edit and add columns which describe the landings recorded on the first ticket. First, replace nominal species IDs (*nominal species id that were not shared: rougheye + blackspot (RBR1)*).
```{r nom_ID}
rawdat.sub2 <- rawdat.sub1 %>%
  mutate(spid_recode = recode(as.character(rawdat.sub1$spid), BMO1 = "BMOL", DVR1 = "DOVR", EGL1 = "EGLS", PTR1 = "PTRL", CSL1 = "CSOL",
                                     REX1 = "REX", RSL1 = "RSOL", SFL1 = "STRY", SSO1= "SSOL", LDB1 = "LDAB", PDB1 = "PDAB", SDB1 = "SSDB", 
                                     ART1 = "ARTH", BSK1 = "BSKT", BLK1 = "BLCK", CNR1 = "CNRY", DBR1 = "DBRK", BLU1 = "BLUR",
                                     BRW1 = "BRWN", CHN1 = "CHNA", CLC1 = "CLCO", COP1 = "COPP", OLV1 = "OLVE", QLB1 = "QLBK", TRE1 = "TREE",
                                     BYL1 = "BYEL", GPH1 = "GPHR", GRS1 = "GRAS", KLP1 = "KLPR", BCC1 = "BCAC", CLP1 = "CLPR", CWC1 = "CWCD",
                                     BRZ1 = "BRNZ", CML1 = "CMEL", GBL1 = "GBLC", GSP1 = "GSPT", GSR1 = "GSRK", HNY1 = "HNYC", MXR1 = "MXRF",
                                     PNK1 = "PNKR", PRR1 = "PRRK", ROS1 = "ROSY", RST1 = "RSTN", SPK1 = "SPKL", SQR1 = "SQRS", STL1 = "STRK",
                                     STR1 = "STAR", SWS1 = "SWSP", TGR1 = "TIGR", VRM1= "VRML", SNS1 = "SNOS", SRK1 = "SRKR", ARR1 = "ARRA",
                                     BGL1 = "BLGL", BNK1 = "BANK", RDB1 = "RDBD", SBL1 = "SBLY", SCR1 = "SCOR", FLG1 = "FLAG", YTR1 = "YTRK",
                                     POP2 = "POP", LSP1 = "LSPN", SSP1 = "SSPN", THD1 = "THDS", WDW1="WDOW", YEY1 = "YEYE", CBZ1 = "CBZN",
                                     KGL1 = "KLPG", LCD1 = "LCOD", CHL1="CHLB", RGL1 = "RCKG", SHP1 = "SHPD"))
```
<br>

```{r include=FALSE}
## clean up space
rm(rawdat.sub1)
```
<br>

Next add columns which describe the port group for landings, based on IOPAC port codes, and species group, based on Holland CCIEA ESR species groupings.


```{r nom_ID_2}
rawdat.sub3 <- rawdat.sub2 %>%
  left_join(
    port_grps, by = c("pcid" = "PACFIN_PORT_CODE")
  ) %>%
  left_join(
    sp_grps, by = c("spid" = "SPID")
  )
```
<br>

```{r include=FALSE}
## clean up space
rm(rawdat.sub2)
```
<br>

Here, include a step to manually adjust for inflation. This step ensures we adjust to a recent and known year.

Adjust revenue for inflation using Fred GDP data, adapted from R code by Erin Steiner and Brooke Hawkins. This product uses the FRED® API but is not endorsed or certified by the Federal Reserve Bank of St. Louis.

Pre-requisite: Create a FRED account and API key. See https://fred.stlouisfed.org/docs/api/api_key.html.


Also, based on Brooke Hawkins' advice, I use the keyring package so that the API key for FRED is not entered as part of the script. The documentation suggests install with the library pak, so that Linux system requirements are automatically installed. Once keyring is installed, you’ll need to save your FREDR API key. This script assumes the name of that secret will be "fredr-api".

In order to save your password, run this command:

key_set("fredr-api")
Enter your FRED API key when prompted for a password and hit enter
Now your key can be retrieved without hardcoding it in the script.



1. Load inflation adjustment factors from FRED. Use annual instead of monthly
```{r inflation-adjustment-download}
# insert your FRED API key
fredr_set_key(key_get("fredr-api"))

# download the quarterly inflation adjustments from Fred from 1985 to present
fred_gdpdefl <- fredr(
    series_id = "GDPDEF",
    observation_start = as.Date(paste0(min(years), "-01-01"))
  )

# generate the mean annual deflators based on the quarterly values
gdp_defl <- mutate(fred_gdpdefl, year = year(date)) %>%
  group_by(year) %>%
  summarize(defl = mean(value), .groups = 'drop') %>%
  mutate(inflation_adjustment_factor = defl / defl[year == max(year)]) %>%
  select(year, inflation_adjustment_factor)

# write inflation adjustment factors
write.csv(file = paste(here(processdir), "inflation_adjustment.csv", sep = "/"), x = gdp_defl, row.names = FALSE)

# plot inflation adjustment factors
gdp_defl %>% ggplot(aes(x = as.factor(year), y = inflation_adjustment_factor)) + geom_point()

# clean up
rm(fred_gdpdefl)
```

2. Add new column for AFI revenue manual and AFI ppp manual
```{r inflation-adjustment}

rawdat.sub4 <- rawdat.sub3 %>% 
  left_join(gdp_defl, by = join_by(year)) %>%
  mutate(afi_revenue_manual = revenue/inflation_adjustment_factor,
         afi_ppp_manual = ppp/inflation_adjustment_factor)

# # check how much of a difference this actually makes
# 
# check_afi_rev <- rawdat.sub4 %>% 
#   mutate(afi_diff = afi_revenue_manual - afi_revenue,
#          afi_ppp_diff = afi_ppp_manual - afi_ppp)
# 
# mean(check_afi_rev$afi_diff, na.rm = TRUE)
# mean(check_afi_rev$afi_ppp_diff, na.rm = TRUE)
# 
# hist(check_afi_rev$afi_diff)
# hist(check_afi_rev$afi_ppp_diff)

```

```{r include=FALSE}
## clean up space
rm(rawdat.sub3)
```
<br>

<br>

Next, add in an estimated exvessel revenue for commercial fish tickets which have no revenue recorded. To do so, we calculate an average price per pound for each species, for every year / week / port group. Then we create an adjusted `ppp` column for trips without revenue, and calculate the `adj_revenue` by multiplying `adj_ppp * pounds`. 

NOTE FROM JS 010522: MF CHECKED AND THIS PPP CHUNK WORKS PROPERLY

```{r adjust_revenue}
## get the average ppp for each species/year/week/port group, write out 
ppp_key <- rawdat.sub4 %>%
  filter(afi_ppp_manual > 0) %>%
  group_by(year, tweek, spid_recode, pcgroup) %>%
  summarise(
    avg_afi_ppp = mean(afi_ppp_manual, na.rm=TRUE),
    .groups = "drop")
if(write_ppp){write.csv(ppp_key, paste0(processdir, "Price_per_Pound_Key_",paste0(years,collapse="-"), ".csv"), row.names=FALSE)}

## recalculate revenue for each ticket
rawdat.sub5 <- rawdat.sub4 %>%
  mutate(
    adj_afi_ppp = ifelse(removal_type_code %in% c("C","D"),
                     ifelse(ppp != 0, afi_ppp_manual, 
                            as.numeric(filter(ppp_key, year == year & tweek == tweek & spid_recode == spid_recode & pcgroup == pcgroup)$avg_afi_ppp)),afi_ppp_manual)
    ) %>%
  mutate(
    adj_afi_revenue = ifelse(afi_revenue_manual != 0, afi_revenue_manual, adj_afi_ppp*pounds)
    )

cat("Added in ", sum(!is.na(rawdat.sub5$adj_afi_ppp)) - sum(!is.na(rawdat.sub5$afi_ppp_manual)), " adjusted ppp.") # this is the modified check Jameal put in
# cat("Added in ", sum(!is.na(rawdat.sub$ppp)) - sum(rawdat.sub$ppp != 0), " adjusted ppp.") # this is the original check Mary wrote
    
```
<br>

```{r include=FALSE}
## clean up space
rm(rawdat.sub4)
```
<br>

# 2: Write out fish tickets

## 2.1. By year
```{r write, eval=TRUE}
for(y in years){
  ## grab data for given year, reorder columns
  tmp_out <- rawdat.sub5 %>%
    filter(year == y) %>%
    dplyr::select(c(trip_id, year, tdate,agid, 
                       pcgroup, pcid, IOPAC,
                       spid, spid_recode, SPGRPN2, council,
                       grgroup, grid, removal_type,removal_type_code,
                       drvid, drvid_year,proc, fleet, participation_group, pounds, ppp, afi_ppp, adj_afi_ppp, revenue, afi_revenue, adj_afi_revenue)) # removed adj_ppp and adj_revenue 01-18-2024
  write.csv(tmp_out, paste0(out_conf_dir, paste0("fish_tickets_", y, "_processed_for_networks_",Sys.Date(),".csv")), row.names=FALSE)
}
```
<br>

## 2.2. By crab year

A crab year is defined as week 46 of year 1 through week 45 of year 2; for example, the 2025 crab year refers to Nov 2024 thru Nov 2025. We use crab year to split data frame, but remove the crab_year column from data set before writing out to file.
```{r write_crab, eval=TRUE}
if(crab_year_write_out_trigger){
  ## create "crab_year" column
  rawdat.sub6 <- rawdat.sub5 %>%
    mutate(crab_year = ifelse(tweek < 46, year, year + 1))
  ## for each crab year...
  for(y in crab_years){
    ## subset the data frame
    tmp_out <- rawdat.sub6 %>%
      filter(crab_year == y) %>%
      dplyr::select(c(trip_id, year, crab_year, tdate,agid, 
                       pcgroup, pcid, IOPAC,
                       spid, spid_recode, SPGRPN2, council,
                       grgroup, grid, removal_type,removal_type_code,
                       drvid, drvid_year,proc, fleet, participation_group, pounds, afi_ppp, adj_afi_ppp, revenue, afi_revenue, adj_afi_revenue)) # removed adj_ppp and adj_revenue 01-18-2024
    ## write out
    write.csv(tmp_out, paste0(out_conf_dir, paste0("fish_tickets_crab", y, "_processed_for_networks_",Sys.Date(),".csv")), row.names=FALSE)
  }
}
```
<br>


---

Script runtime: 
```{r echo=FALSE}
Sys.time()-script_start_time
```
